{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mini_football_env import MiniFootballEnv\n",
    "from mini_football_agent import MiniFootballAgent\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mini Football Environment\n",
    "This will open the Unity Game. \n",
    "\n",
    "For Windows: './mini_football_windows/Mini Football Environment.exe'\n",
    "\n",
    "For Mac: './mini_football_mac/Mini Football Environment.app'\n",
    "\n",
    "The channel params will alow us to run faster experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_path = './mini_football_windows/Mini Football Environment.exe'\n",
    "mac_path = './mini_football_mac/Mini Football Environment.app'\n",
    "\n",
    "# Make Environment\n",
    "env = MiniFootballEnv(path=mac_path)\n",
    "\n",
    "# Set channel config\n",
    "env.channel.set_configuration_parameters(time_scale = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Agent\n",
    "This shows how to load an agent that was trained directly in Unity using PPO.\n",
    "The \"brain\" (the neural network used as function approximator) is stored as an `.onnx` file in `trained_brains/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Agent\n",
    "agent = MiniFootballAgent()\n",
    "agent.load(path=\"trained_brains/FootballPlayer.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Agent in Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward:  0.9143479852027667\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "\n",
    "    terminated = False\n",
    "    steps = 0\n",
    "    episode_reward = 0\n",
    "    observation = env.state\n",
    "\n",
    "    while not terminated:\n",
    "        \n",
    "        action = agent.act(observation)\n",
    "        \n",
    "        # Skip 5 Frames\n",
    "        for _ in range(5):\n",
    "            \n",
    "            observation, reward, terminated, _, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "    all_rewards.append(episode_reward)\n",
    "\n",
    "env.close()\n",
    "print(\"Average Reward: \", np.mean(all_rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-unity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56b88cc66ce22b069d89fcd003cc6f13ce5bcd921cd27810ce44aaafd5c8bba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
